{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class SearcherIIndexVII(SearcherIIndex):\n",
    "    \"\"\"倒排索引文本搜索实现类(改进)\n",
    "    \n",
    "    自定义解析，保留英文片段，将中文片段多粒度分词处理\n",
    "    \n",
    "    Attributes:\n",
    "        index: 检索使用的倒排索引\n",
    "        max_id: 当前索引的文档最大ID\n",
    "        doc_list: 索引文档原文\n",
    "    \"\"\"\n",
    "    def parse_doc(self, doc):\n",
    "        \"\"\"对文档进行自定义解析，保留英文串，对中文串多粒度分词\n",
    "        \n",
    "        Args:\n",
    "            doc:待解析的原始文档\n",
    "        \n",
    "        Returns:\n",
    "            解析结果列表，元素是切分得到的term\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        state_last = ''\n",
    "        cache = ''\n",
    "        for c in doc:\n",
    "            state_c = c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.')\n",
    "            if c == ' ':\n",
    "                if state_last:\n",
    "                    result.append(cache)\n",
    "                else:\n",
    "                    result.extend(list(jieba.cut_for_search(cache)))\n",
    "                result.append(' ')\n",
    "                cache = ''\n",
    "                state_last = '' \n",
    "            else:\n",
    "                if state_c == state_last:\n",
    "                    cache += c\n",
    "                else:\n",
    "                    if state_last != '':\n",
    "                        result.append(cache)\n",
    "                    else:\n",
    "                        result.extend(list(jieba.cut_for_search(cache)))\n",
    "                    cache = c\n",
    "                state_last = state_c\n",
    "        if cache:\n",
    "            if state_last:\n",
    "                result.append(cache)\n",
    "            else:\n",
    "                result.extend(list(jieba.cut_for_search(cache)))\n",
    "        return result\n",
    "    \n",
    "    def add_doc(self, doc):\n",
    "        \"\"\"向索引中添加新文档\n",
    "        \n",
    "        Args:\n",
    "            doc:待检索的文档(文本)\n",
    "        \n",
    "        Returns:\n",
    "            新增文档ID\n",
    "        \"\"\"\n",
    "        self.doc_list.append(doc)\n",
    "        for term in self.parse_doc(doc):\n",
    "            #构建和更新各Term对应的Posting(集合)\n",
    "            if term in self.index: \n",
    "                self.index[term].add(self.max_id)\n",
    "            else:\n",
    "                self.index[term] = set([self.max_id])\n",
    "        self.max_id += 1\n",
    "        return self.max_id - 1\n",
    "    \n",
    "    def dumpIndex(self):\n",
    "        \"\"\"原样输出索引，用于检查索引构建结果\n",
    "        \n",
    "        Returns:\n",
    "            对索引(字典结构)的Dump输出\n",
    "        \"\"\"\n",
    "        print(self.index)\n",
    "    \n",
    "    def parse_query(self, doc):\n",
    "        \"\"\"对查询进行自定义解析，保留英文串，对中文串分词处理\n",
    "        \n",
    "        Args:\n",
    "            doc:待解析的原始文档\n",
    "        \n",
    "        Returns:\n",
    "            解析结果列表，元素是带有串类型标记(首字符，e为英文，c为中文)\n",
    "            的切分term结果\n",
    "        \"\"\"\n",
    "        doc = doc + ' '\n",
    "        result = []\n",
    "        state_last = ''\n",
    "        cache = ''\n",
    "        for c in doc:\n",
    "            state_c = c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.')\n",
    "            flag = None\n",
    "            #增加串标记，e为英文，c为中文(未来可能扩充)\n",
    "            if state_c:\n",
    "                flag = 'e'\n",
    "            else:\n",
    "                flag = 'c'\n",
    "                \n",
    "            if c == ' ':\n",
    "                if state_last != '':\n",
    "                    result.append(cache)\n",
    "                    result.append('s ')\n",
    "                    cache = ''\n",
    "                    state_last = '' \n",
    "            elif c == '(' :\n",
    "                result.append('s' + c)\n",
    "                if cache != '':\n",
    "                    result.append(cache)\n",
    "                    cache = ''\n",
    "                state_last = ''\n",
    "            elif  c == ')':\n",
    "                if cache != '':\n",
    "                    result.append(cache)\n",
    "                    result.append('s' + c)\n",
    "                    cache = ''\n",
    "                state_last = ''\n",
    "            else:\n",
    "                if state_c == state_last:\n",
    "                    cache += c\n",
    "                else:\n",
    "                    if state_last != '':\n",
    "                        result.append(cache)\n",
    "                    cache = flag + c\n",
    "                state_last = state_c\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def conv_query(self, query):\n",
    "        \"\"\"将用户的查询转换成用eval可运行、返回结果ID集合的代码段\n",
    "        \n",
    "        Args:\n",
    "            query:待转换的原始查询字符串\n",
    "        \n",
    "        Returns:\n",
    "            转换完成可通过eval执行返回ID集合的代码段字符串\n",
    "        \"\"\"\n",
    "        query_new_parts = []\n",
    "        all_parts = list(self.parse_query(query))\n",
    "        idx = 0\n",
    "        cache = '' #缓存变量，用于回收分词过程被切开的短语片段\n",
    "        count_parts = len(all_parts)\n",
    "        while idx < count_parts:\n",
    "            if all_parts[idx][1:] == '(' or all_parts[idx][1:] == ')':\n",
    "                query_new_parts.append(all_parts[idx][1:])\n",
    "            elif all_parts[idx][1:] == ' ':\n",
    "                query_new_parts.append(' ')\n",
    "            elif all_parts[idx][1:] in ('and', 'AND', '+'):\n",
    "                query_new_parts.append('&')\n",
    "            elif all_parts[idx][1:] in ('or', 'OR'):\n",
    "                query_new_parts.append('|')\n",
    "            elif all_parts[idx][1:] in ('not', 'NOT', '-'):\n",
    "                query_new_parts.append('-')\n",
    "            elif (idx + 1 < count_parts #被分词切开的短语部分回收至缓存\n",
    "                  and all_parts[idx+1][1:] not in (' ', ')')): \n",
    "                cache += all_parts[idx][1:]\n",
    "            elif (idx + 2 < count_parts #处理词间空格的形式\n",
    "                  and all_parts[idx+1][1:] == \" \" \n",
    "                  and all_parts[idx+2][1:] not in ('(', ')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ')): \n",
    "                query_new_parts.append(\"{} & \".format(self.conv_part(all_parts[idx])))\n",
    "                idx += 2\n",
    "                continue\n",
    "            else:\n",
    "                query_new_parts.append(self.conv_part(cache + all_parts[idx]))\n",
    "                cache = '' #合并完成清空缓存\n",
    "            idx += 1\n",
    "        query_new = ''.join(query_new_parts)\n",
    "        return query_new\n",
    "    \n",
    "    def term_match(self, term):\n",
    "        return self.index.get(term, set()) \n",
    "    \n",
    "    def conv_part(self, part):\n",
    "        flag = part[0]\n",
    "        if flag == 'e':\n",
    "            return \"self.term_match('{}')\".format(part[1:])\n",
    "        elif flag == 'c':\n",
    "            return \"(self.term_match('{}'))\".format(\n",
    "                \"') & self.term_match('\".join(jieba.cut(part[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SearcherIIndexVII('titles.txt')\n",
    "\n",
    "query = '3-0 (中国美国)'\n",
    "print(searcher.parse_query(query))\n",
    "print(searcher.conv_query(query))\n",
    "result = searcher.search(query)\n",
    "if result:\n",
    "    for doc in result:\n",
    "        display(HTML(doc))\n",
    "else:\n",
    "    print('No result.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
